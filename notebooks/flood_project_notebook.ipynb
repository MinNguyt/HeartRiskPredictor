{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f3fe77",
   "metadata": {},
   "source": [
    "# Flood forecasting project — Hanoi (Synthetic dataset)\n",
    "This notebook contains:\n",
    "1. Instructions to replace synthetic data with real downloads\n",
    "2. Preprocessing and feature engineering\n",
    "3. Baseline XGBoost training and evaluation (TimeSeriesSplit)\n",
    "4. Saving model and inference example\n",
    "\n",
    "Files generated with this notebook (synthetic):\n",
    "- `/mnt/data/hanoi_synthetic_2024.csv` (synthetic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bfafb5",
   "metadata": {},
   "source": [
    "## Download real data (optional)\n",
    "If you have access to real rainfall and water level data for Hanoi, replace the synthetic CSV with your real dataset.\n",
    "\n",
    "Useful sources to fetch real data (examples):\n",
    "\n",
    "- GPM IMERG (satellite precipitation) — https://gpm.nasa.gov\n",
    "- CHIRPS gridded rainfall — https://www.chc.ucsb.edu/data/chirps\n",
    "- Vietnam NCHMF / local provincial hydrology portals\n",
    "\n",
    "If you have station CSVs, ensure they have columns: `timestamp`, `station_id`, `rain_mm`, `water_level_cm`, `lat`, `lon`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1581bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load synthetic data\n",
    "csv_path = '/mnt/data/hanoi_synthetic_2024.csv'\n",
    "df = pd.read_csv(csv_path, parse_dates=['timestamp'])\n",
    "print('Rows:', len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9190fcd1",
   "metadata": {},
   "source": [
    "## Preprocessing & Feature Engineering\n",
    "We will create lag features for rain (1,3,6,12,24 hours), rolling sums and an API-like feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72599078",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(['station_id','timestamp']).reset_index(drop=True)\n",
    "for lag in [1,3,6,12,24]:\n",
    "    df[f'rain_lag_{lag}'] = df.groupby('station_id')['rain_mm'].shift(lag)\n",
    "for window in [3,6,24]:\n",
    "    df[f'rain_sum_{window}h'] = df.groupby('station_id')['rain_mm'].rolling(window).sum().reset_index(0,drop=True)\n",
    "# API-like (EWMA)\n",
    "df['API'] = df.groupby('station_id')['rain_mm'].apply(lambda x: x.ewm(alpha=0.3).mean())\n",
    "# dropna\n",
    "df_feat = df.dropna().reset_index(drop=True)\n",
    "print('After creating features, rows:', len(df_feat))\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23abd246",
   "metadata": {},
   "source": [
    "## Baseline: XGBoost (time-series split)\n",
    "We'll use TimeSeriesSplit for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814de1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score\n",
    "import xgboost as xgb\n",
    "\n",
    "features = [c for c in df_feat.columns if c.startswith('rain_lag_') or c.startswith('rain_sum_') or c in ['API']]\n",
    "X = df_feat[features]\n",
    "y = df_feat['label']\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "metrics = []\n",
    "for train_idx, test_idx in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=100, max_depth=5)\n",
    "    model.fit(X_train, y_train)\n",
    "    prob = model.predict_proba(X_test)[:,1]\n",
    "    pred = (prob >= 0.5).astype(int)\n",
    "    metrics.append({\n",
    "        'auc': roc_auc_score(y_test, prob),\n",
    "        'f1': f1_score(y_test, pred),\n",
    "        'recall': recall_score(y_test, pred),\n",
    "        'precision': precision_score(y_test, pred)\n",
    "    })\n",
    "import pandas as pd\n",
    "print(pd.DataFrame(metrics).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7c1abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# train on all data\n",
    "final_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=150, max_depth=5)\n",
    "final_model.fit(X, y)\n",
    "with open('/mnt/data/xgb_flood_model.pkl','wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "print('Saved model to /mnt/data/xgb_flood_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3c3057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference example: take last row features\n",
    "last = X.iloc[[-1]]\n",
    "prob = final_model.predict_proba(last)[:,1][0]\n",
    "print('Predicted probability of flood (last timestamp):', prob)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
